\section{Related Works}
We consider related abstraction libraries in the domain of heterogeneous computing. This is done to identify relevant key points that can help define the library we will be developing.

The libraries are being considering in regards to:
\begin{description}
\item[Goals] \hfill \\
To identify the motivation for the library and to understand the motivation behind its design choices.
\item[Programming Model] \hfill \\
To identify the programming model of a library to see which aproaches have been tried, and what is currently possible. To give a demonstration of the programming model, an implementation of the \textit{SAXPY} computation is presented for each. We chose \textit{SAXPY} due to its simplicity.
\item[Implementation] \hfill \\
To identify the means of facilitating the programming model, showing how it can be done.
\item[Cognitive Dimensions of Notations] \hfill \\
To get a general idea of strengths and weaknesses of the library.
\item[Key Points] \hfill \\
To identify relevant points to note from a library, that should be considered when designing our library.
\end{description}

\todo[inline]{Argumenter for hvorfor vi har valgt at se p√• disse libraries}



\subsection{Thrust}
Thrust is a template library indended to allow developers to implement high performance applications with minimal programming effort. This section is based on Thrust's overview document\cite{thrustOverview} and github page\cite{thrustGithub}.

\subsubsection{Goals}
Thrust is intended to make high performance application development as easy as possible. It is designed to be similar to STL, with intention of being concise, readable, and efficient. It is inteded to supply the developers with containers and fundamental algorithms, with user defined behavior, rather than specific numeric algorithms as provided by BLAS. It is also intended to be interoperable with CUDA.

\subsubsection{Programming Model}
Bolt is modeled on STL and as such, follows the model of calling functions with iterators as arguments to instruct where input, and output is located.

In listing \ref{code:thrustSaxpy} we show how saxpy can be implemented in Thrust, and the usage of iterators to manage data access is shown.

The execution of saxpy is done in line \ref{code:thrustSaxpy:execute}, and shows how we use iterators to define input and output location.
\begin{lstlisting}[caption={Thrust saxpy example}, label={code:thrustSaxpy}]
size_t N = 1024;
int a = 10;

//cuda classifier on lambdaz
auto func = [=]__device__(int x, int y){return a * x + y;};

//initialize host vectors
thrust::host_vector<int> x(N);
thrust::host_vector<int> y(N);
thrust::host_vector<int> z(N);

//fill with random data
std::generate(x.begin(), x.end(), rand);
std::generate(y.begin(), y.end(), rand);

//copy to device
thrust::device_vector<int> d_x = x;
thrust::device_vector<int> d_y = y;

//perform saxpy
thrust::transform(d_x.begin(), d_x.end(), d_y.begin(), d_y.begin(), func); ~\label{code:thrustSaxpy:execute}~

//copy results back to host vector
z = d_y;
\end{lstlisting}

\subsubsection{Implementation}
Thrust is a library with abstractions build on top of CUDA.

\subsubsection{Cognitive Dimensions of Notations}

\subsubsection{Key Points}
The key points of Thrust is:

\begin{itemize}
\item API immitating STL.
\item Being purely a library on top of CUDA.
\item Allowing mixing with CUDA code. 
\end{itemize}

The usage of the containers is based on iterators, as STL is, and can be very verbose with multiple operations on the same container.

% ---=== SkelCL ===--- %

\subsection{SkelCL}
SkelCL (Skeleton Computing Language) is a library aiming to provide abstractions for parallel programming on multi GPU systems. It is developed as a research project by Michel Steuwer et.al at University of Munster. This section is witten based upon the information available on their website \cite{skelclWebsite} and their paper \cite{skelclPaper}.

\subsubsection{Goals}
The developers of SkelCL believe that programming for GPUs results in complex, lengthy and error prone programs. This is due to the process of writing GPU code typically being reliant on low level programming aproaches as seen with OpenCL and CUDA. 

To avoid the pitfalls of the traditional low level aproaches, the library SkelCL provides abstractions in the form of algorithmic patterns, parallel container data types, and handling of transfers between host and device. 

SkelCl can be used on single GPU systems, but is mainly aimed at systems worth multipe GPUs and provide a feature called \textit{data (re)distributions} which manages data among the available GPUs.

\subsubsection{Programming Model}
The programming model is centered around \textit{parallel skeletons}, which is pre-implemented high level patterns that can be customized for a given problem. The available skeletons are \textit{map}, \textit{zip}, \textit{reduce}, \textit{scan}, \textit{mapOverlap}, end \textit{allpairs}.

A computation of the \textit{SAXPY} computation in SkelCL is shown in listing \ref{code:skelclSample}. After SkelCL is initialised, which happens at line three, skeletons can be constructed. The \texttt{Zip} skeleton have been used and are specified by the provided paremeters; \texttt{<float(float,float)>} indicates that the resulting \texttt{Zip} function expects two floats and a single float will be returned. The given string specifies the function of the skeleton. In line 13 the calculation is performed based on the constructed skeletons.

\begin{lstlisting}[caption={Computation of the dot product of two vectors}, label=code:skelclSample] 
using namespace skelcl;
int main() {
  skelcl::init();

  Zip<float(float,float)> saxpy("float func(float x, float y, float a){ return a*x + y; }");

  Vector<float> X(1024);      
  Vector<float> Y(1024);
  init(X.begin(), X.end()); 
  init(Y.begin(), Y.end());
  float a = 2.5f;

  saxpy( out(Y), X, Y, a );
}
\end{lstlisting}

\subsubsection{Implementation}
SkelCL is a library that is built upon OpenCL. This allows host and kernel code to be contained within one source file, as opposed to the traditional OpenCL approach.

\subsubsection{Cognitive Dimensions of Notations}
\todo[inline]{Empty for now}

\subsubsection{Key points}
A key point of OpenCL is the data containers it provides, namely vectors and matrices. They are transparently available on both host and device. When one of these data containers are allocated or deallocated on the host, it is automatically also allocated or dealocated on the device(s). Futhermore, memory transfers between host and device are handled implicitly.

Another key point of SkelCL is how it is designed to function on systems with multiple GPUs. The \textit{disctribution mechanism} that OpenCL provides describes how a container is distributed among the available GPUs. This feature abstracts away the need to manage what parts of the container gets assigned to which GPU. The data containers can be considered as self contained entities. The programmer has to specify a model for how the data should be distributed, with the available options being \textit{single}, \textit{copy}, \textit{block}, and \textit{overlap}.

% ---=== Bolt ===--- %

\subsection{Bolt}
Bolt is a library providing abstractions for heterogeneous computing. This section is based on Bolt's documentation\cite{boltDoc} and github page\cite{boltGithub}.

A unique feature of Bolt is the possibility to run its algoritms on either CPU or GPU without the same code.

\subsubsection{Goals}
Bolt is designed to provide high performance library implementations for common algorithms, following the structure of STL. It is intended to make heterogeneous development easier.

It is designed to provide an application that can execute on either a CPU or any OpenCL capable unit.

\subsubsection{Programming Model}
Bolt is modeled on STL and as such, follows the model of calling functions with iterators as arguments to instruct where input, and output is located.

It have functions for modifying STL containers, and then the library decides whether the computation should happen on the host or the device, involving any copy needed to do so.

The example shown in listing \ref[{code:boltSaxpy}], shows how well the library interfaces with an STL vector.

From line \ref{code:boltSaxpy:cppamp} until the next comment it is shown how the function is defined and implemented done with the C++ Amp backend. It is done with a c++11 lambda and C++ Amp's restrict classifier.

From line \ref{code:boltSaxpy:opencl} it is shown how, instead of a lambda, a functor is needed. The functor is then defined inside a BOLT\_FUNCTOR macro to statically generate relevant OpenCL code.

\begin{lstlisting}[caption={Bolt saxpy example}, label={code:boltSaxpy}]
const size_t N = 1024;
int a = 10;

std::vector<int> x(N);
std::vector<int> y(N);
std::vector<int> z(N);

std::generate(x.begin(), x.end(), rand);
std::generate(y.begin(), y.end(), rand);

//bolt with c++ amp backend ~\label{code:boltSaxpy:cppamp}~
auto saxpyLambda = [=] (float xx, float yy) restrict(cpu,amp) {
  return a * xx + yy;
};
bolt::transform(x.begin(), x.end(), y.begin(), z.begin, saxpyLambda);

//bolt with opencl backend ~\label{code:boltSaxpy:opencl}~
BOLT_FUNCTOR(SaxpyFunctor,
  struct SaxpyFunctor{
    int _a;
    SaxpyFunctor(int a): _a(a) {};
    float operator() (const int& xx, const int& yy){
      return _a * xx + yy;
    };
  };
);
boltcl::transform(x.begin(), x.end(), y.begin(), z.begin, SaxpyFunctor(a));
\end{lstlisting}

\subsubsection{Implementation}
Bolt is a library on top of either C++ Amp or OpenCL. The API is the same, while the supported features changes depending on what implementation is selected.
As shown in the example, only the C++ Amp backend supports c++11 lambdas, which reduces the amount of work needed by the developer, compared to the functor.
%https://www.slideshare.net/hsafoundation/bolt-for-hsa-by-ben-sanders

\subsubsection{Key Points}
Being able to use the same algorithms on both STL containers, and Bolts containers can provide an easier transition.

Being a library on top of other frameworks results in some code artefacts. With C++ Amp as target, the use of the restrict classifier on lambdas seem unergonomic. With OpenCL as target, it is the use of BOLT_FUNCTOR as a macro to overcome the language gap between C++ and OpenCL C. Both examples show that workarounds to support the target sometimes will show up in the API. 

The usage of the containers is based on iterators, as STL is, and can be very verbose with multiple operations on the same container.

% ---=== SYCL ===--- %
//bolt with c++ amp backend ~\label{code:boltSaxpy:cppamp}~
auto saxpyLambda = [=] (float xx, float yy) re

\subsection{SYCL}
\textit{SYCL} is a high-level programming language that provide an abstraction layer for \textit{OpenCL} and it is developed by Khronos group. This section describes \textit{SYCL} based on the information available at the Khronos website REF.

\subsubsection{Goals}
As opposed to regular \textit{OpenCL} devlopment, \textit{SYCL} enables the host and device code to be contained within a single source. \textit{SYCL} exposes the \textit{OpenCL} feature-set with a higher abstraction level, as well as most modern \textit{C++} features. 

The Khronos group aim to follow the current C++ standard developments and integrate it with OpenCL features.

\subsubsection{Programming model}
\textit{OpenCL} has commands for memory object creation, copying, mapping and synchronisation. \textit{SYCL} wraps these as a \textit{command group} that can manage these commands. Listing \ref{code:saxpySycl} shows a sample with an implementation of saxpy. \textit{SYCL} need to know which variables should be available for device use, and this is indicated by firstly setting up host storage as seen on line one to three. These variables, \texttt{x}, \texttt{y}, and \texttt{z}, are then placed in a buffer, as seen on lines five to seven, which marks them data to be shared between host and device and initializes the queue. Next, at lines nine and ten, the available decises are registered by initializing the \texttt{device\_selector}. At line 12 the buffer elements are submited by the \texttt{cgh} handler, and at following three lines it is specified how each buffer element should be accessed; \texttt{x} and \texttt{y} is set with the \textit{read} access mode and \texttt{z} is set with the \textit{discard\_write} access mode. The actual execution of saxpy is specified at lines 17 and 18.

\begin{lstlisting}[caption={Saxpy implemented in SYCL}, label={code:saxpySycl}]
sycl::float4 x = { 1.0, 2.0, 3.0, 4.0 };
sycl::float4 y = { 2.0, 3.0, 4.0, 5.0 };
sycl::float4 z = { 0.0, 0.0, 0.0, 0.0 };

sycl::buffer<sycl::float4, 1> a_sycl(&x, sycl::range<1>(1));
sycl::buffer<sycl::float4, 1> a_sycl(&y, sycl::range<1>(1));
sycl::buffer<sycl::float4, 1> a_sycl(&z, sycl::range<1>(1));

sycl::default_selector device_selector;
sycl::queue queue(device_selector);

queue.submit([&] (sycl::handler& cgh) {
  auto x_acc = x_sycl.get_access<sycl::access::mode::read>(cgh);
  auto y_acc = y_sycl.get_access<sycl::access::mode::read>(cgh);
  auto z_acc = z_sycl.get_access<sycl::access::mode::discard_write>(cgh);

  cgh.single_task<class saxpy>([=] () {
    z_acc[0] = 2 * x_acc[0] + y_acc[0];
  });
});
\end{lstlisting}

Even though SYCL provides a higher abstraction level compared to regular \textit{OpenCL}, low-level \textit{C++} and \textit{OpenCL} features are still available.

\subsubsection{Implementation}
Primarily targets \textit{OpenCL}, but can also target other backends as it is translated to \textit{SPIR-V}, which an imediate representation.

\subsubsection{Key Points}
A key point of \textit{SYCL} to make note of is its way of specifying data that should be available for host and devices. By placing data in the buffers and specifying how the data should be accessed, shown in listing \ref{code:saxpySycl} as \texttt{read} and \texttt{discard\_write}, the availability of data is clear. 

Another point to note, that is different than the approaches of the other libraries, is that the parallel logic is structured similar to how a kernel function would be in \textit{OpenCL}/\textit{CUDA} instead of it being abstracted away.

% ---=== C++ AMP ===--- %

\subsection{C++ AMP}
AMP stands for Accelerated Massive Parallelism, and is a runtime library that allows a developer to write code to be excecuted on data-parallel hardware and is built upon \textit{DirectX 11}. \textit{C++ AMP} was initially developed by Microsoft as a library and an open standard for implementing parallelism in \textit{C++}, which have led to the announcement from the HSA Foundation about an AMP compiler built with \textit{Clang} and \textit{LLVM}, that outputs to \textit{OpenCL} instead of DirectX11. The information discussed in this section was gained from Microsoft's \textit{C++ AMP} page \cite{microsoftCppAMP}.

\subsubsection{Goals}
The aim of the \textit{C++ AMP} specification is to provide a way of writing code for data parallel hardware directly within the C++ language. Microsoft implemented the spacification based upon \textit{DirectX 11}, and the HSA Foundation later did it for OpenCL.

\subsubsection{Programming model}
The major thing to note about the programming model of \textit{C++ AMP} is that kernel functions is here expressed in C++ as labdas. 

To construct matrices, a developer will create an array, and wrap it with the \textit{C++ AMP} provided \texttt{array\_view}. To show an example, with an array, \texttt{int matrix[] = \{1, 2, 3, 4\};}, it can be viewed as having multiple dimensions by writing \texttt{array\_view<int, 2> mat(2, 2, matrix);}. Here it was specified that the array should be seen as a matrix of integers with two dimensions. It is then specified that \texttt{mat} have two rows with two elements each, and that it should use the data contained withing \texttt{matrix}.

Listing \ref{code:cppampSaxpy} show \textit{SAXPY} implemented in \textit{C++ AMP}. The \texttt{array\_view}s are constructed at line \ref{code:cppampSaxpy:viewsStart} to \ref{code:cppampSaxpy:viewsEnd}. It is still needed to specify the views, even though we here only utilize one dimension. The \texttt{z\_v} \texttt{array\_view} is at line \ref{code:cppampSaxpy:discard} marked with the \texttt{discard\_data()} function. This is done to indicate that we want to use \texttt{z\_v} purely as an output container, and that we dont want to waste time transferring it to device since the contents will be overwritten anyway.
At line \ref{code:cppampSaxpy:forEach} the function \texttt{parallel\_for\_each()} method is called and given two arguments. \texttt{z\_v.extend} indicates the compute domain. The given lambda are marked with the \textit{C++ AMP} provided \texttt{restrict(amp)} which states that the lambda should be excecuted on device and that only a subset \textit{C++} functionality is available for excecution.
\begin{lstlisting}[caption={\textit{C++ AMP} \textit{SAXPY} example.}, label={code:cppampSaxpy}]
const size_t N = 1024;
int a = 10;

std::array<int, N> x;
std::array<int, N> y;
std::array<int, N> z;

std::generate(x.begin(), x.end(), rand);
std::generate(y.begin(), y.end(), rand);

array_view<const int, 1> x_v(size, x);~\label{code:cppampSaxpy:viewsStart}~
array_view<const int, 1> y_v(size, y);
array_view<int, 1> z_v(size, z);~\label{code:cppampSaxpy:viewsEnd}~
z_v.discard_data();~\label{code:cppampSaxpy:discard}~

parallel_for_each( ~\label{code:cppampSaxpy:forEach}~
    z_v.extent,

    [=](index<1> idx) restrict(amp){
        z_v[idx] = a * x_v[idx] + y_v[idx];
    }
)
\end{lstlisting}

\subsubsection{Implementation}
\textit{C++ AMP} enables parallelism based on \textit{DirectX11}.

\subsubsection{Key Points}


The idea of \texttt{array\_view} that interprets one dimensional arrays is interesting. 



% ---=== PACXX ===--- %

\subsection{PACXX}
PACXX is a unified programming model that uses a custom compiler based on Clang. It is a research project created by Michael Haidl and Sergei Gorlatch both from University of Muensterm Germany. The information in this section is based upon their \textit{PACXX} paper released in 2014 REF SOURCE. \textit{PACXX} is not officially released yet, but the compiler can be found on Github REF SOURCE.

\subsubsection{Goals}
The \textit{PACXX} paper states that the \textit{OpenCL} and \textit{CUDA} are error-prone since with these approaches, host code is writen in \textit{C}/\textit{C++} with a restructed, \textit{C}-like API to handle menory management and device specific code is written as device specific code with a parallel programming model. The aim of \textit{PACXX} is to avoid the praditional pitfalls of GPU programming by unifying host and device code and thereby allowing the programmer to stay, express herself using \textit{C++14} and STL features.

\subsubsection{Programming Model}
As the aim of \textit{PACXX} suggests, the programming model is similar to a regular \textit{C++} approach, such that the developer would will not have to change mindset when programming. There are some exceptions note; The programmer still need to evaluate the threads and blocks she want to use. The programmer must use the \texttt{kernel} class that \textit{PACXX} provides to construct the kernel function. Lastly, \textit{PACXX} genereates and compiles device code at runtime, and there are no restrictins as to whan a kernel function can call, but all related code must be known at runtime. This means that functions from pre-compiled libraries cannot be used by a kernel function.

Listing \ref{code:saxpyPACXX} shows a saxpy implementation using \textit{PACXX}. A lambda function called \texttt{saxpy} is created on line 6, which describes saxpy. The thread id will be fetched, as seen on line 7, and then the elements corresponding to that thread of each vector will be used for the saxpy computation. The amount of threads and blocks are determined at line 12 and 13. Then, at line 15, the kernel function is constructed using the \textit{PACXX} provided \texttt{kernel} class. The saxpy is excecuted at line 16.

\begin{lstlisting}[caption={saxpy made with PACXX.}, label={code:saxpyPACXX}]
int main() {
  size_t = 1 << 24;
  int a = 2;
  std::vector<int> x(n), y(n), z(n);

  auto saxpy = [](const int& a, const vector<int>& x, const vecotr<int>& y, vector<int>& z) {
    auto i = Thread::get().global.x;
    if (i >= x.size()) return;
    z[i] = x[i] * a + y[i];
  };

  size_t threads = 128;
  size_t blocks = (n + (threads * 2 - 1)) / (threads * 2);

  auto saxpy_gpu = kernel(saxpy, {{blocks}, {threads}});
  saxpy_gpu(a, x, y, z);
}
\end{lstlisting}

\subsubsection{Implementation}
\textit{PACXX} utilizes \textit{LLVM} to genereate \textit{PTX} code at runtime. \textit{PACXX} is multi-staged.

\subsubsection{Key Points}
\textit{PACXX} abstacts most of the traditional GPU devopment away since the programmer can now write device code in \textit{C++14}.

\textit{PACXX} ueses \textit{LLVM} to generate and compile code, \textit{PTX}, at runtime. This is interesting since provides more apportunities and freedom for abstractions that a static library would.