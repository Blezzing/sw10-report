\section{Related Works}
We consider related abstraction libraries in the domain of heterogeneous computing. This is done to identify relevant key points that can help define the library we will be developing.

The libraries are being considering in regards to:
\begin{description}
\item[Goals] \hfill \\
To identify the motivation for the library and to understand the motivation behind its design choices.
\item[Programming Model] \hfill \\
To identify the programming model of a library to see which aproaches have been tried, and what is currently possible.
\item[Implementation] \hfill \\
To identify the means of facilitating the programming model, showing how it can be done.
\item[Cognitive Dimensions of Notations] \hfill \\
To get a general idea of strengths and weaknesses of the library.
\item[Key Points] \hfill \\
To identify relevant points to note from a library, that should be considered when designing our library.
\end{description}

\todo[inline]{Argumenter for hvorfor vi har valgt at se på disse libraries}
\todo[inline]{Sig at vi demonstrerer med saxpy og hvorfor vi har valgt saxpy.\\  --== Temp presentation below==--}
The NVIDIA Developer Blog has a post titled 'An Easy Introduction to CUDA C and C++', wich is post a devloper, that are unexperienced in the 
 
\subsection{Thrust}
Thrust is a template library indended to allow developers to implement high performance applications with minimal programming effort. This section is based on Thrust's overview document\cite{thrustOverview} and github page\cite{thrustGithub}.

\subsubsection{Goals}
Thrust is intended to make high performance application development as easy as possible. It is designed to be similar to STL, with intention of being concise, readable, and efficient. It is inteded to supply the developers with containers and fundamental algorithms, with user defined behavior, rather than specific numeric algorithms as provided by BLAS. It is also intended to be interoperable with CUDA.

\subsubsection{Programming Model}
Bolt is modeled on STL and as such, follows the model of calling functions with iterators as arguments to instruct where input, and output is located.

In listing \ref{code:thrustSaxpy} we show how saxpy can be implemented in Thrust, and the usage of iterators to manage data access is shown.

The execution of saxpy is done in line \ref{code:thrustSaxpy:execute}, and shows how we use iterators to define input and output location.
\begin{lstlisting}[caption={Thrust saxpy example}, label={code:thrustSaxpy}]
size_t N = 1024;
int a = 10;

//cuda classifier on lambdaz
auto func = [=]__device__(int x, int y){return a * x + y;};

//initialize host vectors
thrust::host_vector<int> x(N);
thrust::host_vector<int> y(N);
thrust::host_vector<int> z(N);

//fill with random data
std::generate(x.begin(), x.end(), rand);
std::generate(y.begin(), y.end(), rand);

//copy to device
thrust::device_vector<int> d_x = x;
thrust::device_vector<int> d_y = y;

//perform saxpy
thrust::transform(d_x.begin(), d_x.end(), d_y.begin(), d_y.begin(), func); ~\label{code:thrustSaxpy:execute}~

//copy results back to host vector
z = d_y;
\end{lstlisting}

\subsubsection{Implementation}
Thrust is a library with abstractions build on top of CUDA.

\subsubsection{Cognitive Dimensions of Notations}

\subsubsection{Key Points}
The key points of Thrust is:

\begin{itemize}
\item API immitating STL.
\item Being purely a library on top of CUDA.
\item Allowing mixing with CUDA code. 
\end{itemize}

The usage of the containers is based on iterators, as STL is, and can be very verbose with multiple operations on the same container.

\subsection{SkelCL}
SkelCL (Skeleton Computing Language) is a library aiming to provide abstractions for parallel programming on multi GPU systems. It is developed as a research project by Michel Steuwer et.al at University of Munster. This section is witten based upon the information available on their website \cite{skelclWebsite} and their paper \cite{skelclPaper}.

\subsubsection{Goals}
The developers of SkelCL believe that programming for GPUs results in complex, lengthy and error prone programs. This is due to the process of writing GPU code typically being reliant on low level programming aproaches as seen with OpenCL and CUDA. 

To avoid the pitfalls of the traditional low level aproaches, the library SkelCL provides abstractions in the form of algorithmic patterns, parallel container data types, and handling of transfers between host and device. 

SkelCl can be used on single GPU systems, but is mainly aimed at systems worth multipe GPUs and provide a feature called \textit{data (re)distributions} which manages data among the available GPUs.

\subsubsection{Programming Model}
The programming model is centered around \textit{parallel skeletons}, which is pre-implemented high level patterns that can be customized for a given problem. The available skeletons are \textit{map}, \textit{zip}, \textit{reduce}, \textit{scan}, \textit{mapOverlap}, end \textit{allpairs}.

A computation of the dot product of two vectors in SkelCL is shown in listing \ref{code:skelclSample}. After SkelCL is initialised, line three, the skeletons can be constructed. The \texttt{Zip} and \texttt{Reduce} have been used and are specified by the provided paremeters; \texttt{<int(int,int)>} indicates that the function expects two integers and a single integer will be returned. The given string specifies the function of the skeleton. In line 12 the calculation is performed based on the constructed skeletons.

\begin{lstlisting}[caption={Computation of the dot product of two vectors}, label=code:skelclSample] 
using namespace skelcl;
int main() {
  skelcl::init();

  Zip<int(int,int)> mult("int func(int x, int y){ return x*y; }");
  Reduce<int(int)> sum("int func(int x, int y){ return x+y; }", "0");

  Vector<int> a(1024);
  Vector<int> b(1024);
  init(a.begin(), a.end()); init(b.begin(), b.end());

  Vector<int> c = sum( mult(a, b) );

  std::cout << "dot product: " << c.front() << std::endl;
}
\end{lstlisting}

\subsubsection{Implementation}
SkelCL is a library that is built upon OpenCL. This allows host and kernel code to be contained within one source file, as opposed to the traditional OpenCL approach.

\subsubsection{Cognitive Dimensions of Notations}
\todo[inline]{Empty for now}

\subsubsection{Key points}
A key point of OpenCL is the data containers it provides, namely vectors and matrices. They are transparently available on both host and device. When one of these data containers are allocated or deallocated on the host, it is automatically also allocated or dealocated on the device(s). Futhermore, memory transfers between host and device are handled implicitly.

Another key point of SkelCL is how it is designed to function on systems with multiple GPUs. The \textit{disctribution mechanism} that OpenCL provides describes how a container is distributed among the available GPUs. This feature abstracts away the need to manage what parts of the container gets assigned to which GPU. The data containers can be considered as self contained entities. The programmer has to specify a model for how the data should be distributed, with the available options being \textit{single}, \textit{copy}, \textit{block}, and \textit{overlap}.

\subsection{Bolt}
Bolt is a library providing abstractions for heterogeneous computing. This section is based on Bolt's documentation\cite{boltDoc} and github page\cite{boltGithub}.

A unique feature of Bolt is the possibility to run its algoritms on either CPU or GPU without the same code.

\subsubsection{Goals}
Bolt is designed to provide high performance library implementations for common algorithms, following the structure of STL. It is intended to make heterogeneous development easier.

It is designed to provide an application that can execute on either a CPU or any OpenCL capable unit.

\subsubsection{Programming Model}
Bolt is modeled on STL and as such, follows the model of calling functions with iterators as arguments to instruct where input, and output is located.

An example is shown in Listing \ref{code:boltExample1}, where we sort a device vector. This is identical to the method shown in Listing \ref{code:boltExample2}, where a a std::vector is sorted with std::sort.
\begin{lstlisting}[caption={Bolt sort example}, label={code:boltExample1}]
//vector construction
bolt::cl::device_vector<int> input(1024);

//vector fill ommited

//inplace sort
bolt::cl::sort(input.begin(), input.end());
\end{lstlisting}

\begin{lstlisting}[caption={STL sort example}, label={code:boltExample2}]
//vector construction
std::vector<int> input(1024);

//vector fill ommited

//inplace sort
std::sort(input.begin(), input.end());
\end{lstlisting}

\subsubsection{Implementation}
\todo[inline]{Bolt is a library... Er ikke sikker på om det kræver speciel kompilerings process.

Bolt targets... almindelige cpuer og alt der kan køre c++ amp, eller openCL.
}

\subsubsection{Cognitive Dimensions of Notations}
\todo[inline]{we could evaluate all criterion here}

\subsubsection{Key Points}
The key points of Bolt is:

\begin{itemize}
\item The immitation of STL.
\item The capability of single code base working on both CPU and GPU.
\end{itemize}

The usage of the containers is based on iterators, as STL is, and can be very verbose with multiple operations on the same container.

% ---=== SYCL ===--- %

\subsection{SYCL}
\textit{SYCL} is a high-level programming language that provide an abstraction layer for \textit{OpenCL} and it is developed by Khronos group. This section describes \textit{SYCL} based on the information available at the Khronos website REF.

\subsubsection{Goals}
As opposed to regular \textit{OpenCL} devlopment, \textit{SYCL} enables the host and device code to be contained within a single source. \textit{SYCL} exposes the \textit{OpenCL} feature-set with a higher abstraction level, as well as most modern \textit{C++} features. 

The Khronos group aim to follow the current C++ standard developments and integrate it with OpenCL features.

\subsubsection{Programming model}
\textit{OpenCL} has commands for memory object creation, copying, mapping and synchronisation. \textit{SYCL} wraps these as a \textit{command group} that can manage these commands. Listing \ref{code:saxpySycl} shows a sample with an implementation of saxpy. \textit{SYCL} need to know which variables should be available for device use, and this is indicated by firstly setting up host storage as seen on line one to three. These variables, \texttt{x}, \texttt{y}, and \texttt{z}, are then placed in a buffer, as seen on lines five to seven, which marks them data to be shared between host and device and initializes the queue. Next, at lines nine and ten, the available decises are registered by initializing the \texttt{device\_selector}. At line 12 the buffer elements are submited by the \texttt{cgh} handler, and at following three lines it is specified how each buffer element should be accessed; \texttt{x} and \texttt{y} is set with the \textit{read} access mode and \texttt{z} is set with the \textit{discard\_write} access mode. The actual execution of saxpy is specified at lines 17 and 18.

\begin{lstlisting}[caption={Saxpy implemented in SYCL}, label={code:saxpySycl}]
sycl::float4 x = { 1.0, 2.0, 3.0, 4.0 };
sycl::float4 y = { 2.0, 3.0, 4.0, 5.0 };
sycl::float4 z = { 0.0, 0.0, 0.0, 0.0 };

sycl::buffer<sycl::float4, 1> a_sycl(&x, sycl::range<1>(1));
sycl::buffer<sycl::float4, 1> a_sycl(&y, sycl::range<1>(1));
sycl::buffer<sycl::float4, 1> a_sycl(&z, sycl::range<1>(1));

sycl::default_selector device_selector;
sycl::queue queue(device_selector);

queue.submit([&] (sycl::handler& cgh) {
  auto x_acc = x_sycl.get_access<sycl::access::mode::read>(cgh);
  auto y_acc = y_sycl.get_access<sycl::access::mode::read>(cgh);
  auto z_acc = z_sycl.get_access<sycl::access::mode::discard_write>(cgh);

  cgh.single_task<class saxpy>([=] () {
    z_acc[0] = 2 * x_acc[0] + y_acc[0];
  });
});
\end{lstlisting}

Even though SYCL provides a higher abstraction level compared to regular \textit{OpenCL}, low-level \textit{C++} and \textit{OpenCL} features are still available.

\subsubsection{Implementation}
Primarily targets \textit{OpenCL}, but can also target other backends as it is translated to \textit{SPIR-V}, which an imediate representation.

\subsubsection{Cognitive Dimensions of Notations}
\todo[inline]{Empty for now}

\subsubsection{Key Points}
A key point of \textit{SYCL} to make note of is its way of specifying data that should be available for host and devices. By placing data in the buffers and specifying how the data should be accessed, shown in listing \ref{code:saxpySycl} as \texttt{read} and \texttt{discard\_write}, the availability of data is clear. 

Another point to note, that is different than the approaches of the other libraries, is that the parallel logic is structured similar to how a kernel function would be in \textit{OpenCL}/\textit{CUDA} instead of it being abstracted away.

% ---=== C++ AMP ===--- %

\subsection{C++ AMP}
https://msdn.microsoft.com/en-us/library/hh265137.aspx

\subsubsection{Goals}

\subsubsection{Programming model}
saxpy
\begin{lstlisting}[caption={C++ AMP saxpy example}, label={code:cppampSaxpy}]
const size_t N = 1024;
int a = 10;

//source objects
std::array<int, N> x;
std::array<int, N> y;
std::array<int, N> z;

//fill with random data
std::generate(x.begin(), x.end(), rand);
std::generate(y.begin(), y.end(), rand);

//create C++ AMP objects.
array_view<const int, 1> x_v(size, x);
array_view<const int, 1> y_v(size, y);
array_view<int, 1> z_v(size, z);
z_v.discard_data();

parallel_for_each(
    //define the compute domain
    sum.extent,

    //code to run on each thread on accelerator
    [=](index<1> idx) restrict(amp){
        z_v[idx] = a * x_v[idx] + y_v[idx];
    }
)

//z now holds the result
\end{lstlisting}

\subsubsection{Implementation}

\subsubsection{Cognitive Dimensions of Notations}

\subsubsection{Key Points}

% ---=== PACXX ===--- %

\subsection{PACXX}
PACXX is a unified programming model that uses a custom compiler based on Clang. It is a research project created by Michael Haidl and Sergei Gorlatch both from University of Muensterm Germany. The information in this section is based upon their \textit{PACXX} paper released in 2014 REF SOURCE. \textit{PACXX} is not officially released yet, but the compiler can be found on Github REF SOURCE.

\subsubsection{Goals}
The \textit{PACXX} paper states that the \textit{OpenCL} and \textit{CUDA} are error-prone since with these approaches, host code is writen in \textit{C}/\textit{C++} with a restructed, \textit{C}-like API to handle menory management and device specific code is written as device specific code with a parallel programming model. The aim of \textit{PACXX} is to avoid the praditional pitfalls of GPU programming by unifying host and device code and thereby allowing the programmer to stay, express herself using \textit{C++14} and STL features.

\subsubsection{Programming Model}
As the aim of \textit{PACXX} suggests, the programming model is similar to a regular \textit{C++} approach, such that the developer would will not have to change mindset when programming. There are some exceptions note; The programmer still need to evaluate the threads and blocks she want to use. The programmer must use the \texttt{kernel} class that \textit{PACXX} provides to construct the kernel function. Lastly, \textit{PACXX} genereates and compiles device code at runtime, and there are no restrictins as to whan a kernel function can call, but all related code must be known at runtime. This means that functions from pre-compiled libraries cannot be used by a kernel function.

Listing \ref{code:saxpyPACXX} shows a saxpy implementation using \textit{PACXX}. A lambda function called \texttt{saxpy} is created on line 6, which describes saxpy. The thread id will be fetched, as seen on line 7, and then the elements corresponding to that thread of each vector will be used for the saxpy computation. The amount of threads and blocks are determined at line 12 and 13. Then, at line 15, the kernel function is constructed using the \textit{PACXX} provided \texttt{kernel} class. The saxpy is excecuted at line 16.

\begin{lstlisting}[caption={saxpy made with PACXX.}, label={code:saxpyPACXX}]
int main() {
  size_t = 1 << 24;
  int a = 2;
  std::vector<int> x(n), y(n), z(n);

  auto saxpy = [](const int& a, const vector<int>& x, const vecotr<int>& y, vector<int>& z) {
    auto i = Thread::get().global.x;
    if (i >= x.size()) return;
    z[i] = x[i] * a + y[i];
  };

  size_t threads = 128;
  size_t blocks = (n + (threads * 2 - 1)) / (threads * 2);

  auto saxpy_gpu = kernel(saxpy, {{blocks}, {threads}});
  saxpy_gpu(a, x, y, z);
}
\end{lstlisting}

\subsubsection{Implementation}
\textit{PACXX} utilizes \textit{LLVM} to genereate \textit{PTX} code at runtime. \textit{PACXX} is multi-staged.

\subsubsection{Cognitive Dimensions of Notations}
\todo{At some point}

\subsubsection{Key Points}
\textit{PACXX} abstacts most of the traditional GPU devopment away since the programmer can now write device code in \textit{C++14}.

\textit{PACXX} ueses \textit{LLVM} to generate and compile code, \textit{PTX}, at runtime. This is interesting since provides more apportunities and freedom for abstractions that a static library would.