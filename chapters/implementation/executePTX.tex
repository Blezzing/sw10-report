\section{Execute PTX} \label{cha:execPtx}
% Motivation
Executing PTX code on the GPU is a central part of \textit{YAGAL}'s functionality, and the library achieves this by interacting with the \textit{CUDA} driver API. All CUDA related code in \textit{YAGAL} are contained within the \texttt{yagal::cuda} namespace.

% Implementation
To execute the \textit{PTX} code, it is passed to the function \texttt{executePtxWithParams} as a string, along with a vector containing the input parameters as seen on Listing \ref{code:executePtxWithParams}. The last two parameters corresponds to block and grid dimensions. The optimal values for these vary depending on algorithm and data sizes. We chose to default to \texttt{128, 1, 1} for both, as it results in $128*128=16384$ instances of the kernel, which is sufficient to make use of all threads of any current GPU. When tested on sample calculations we found this to be a well performing configuration compared to other configurations. Results from these tests can be found in appendix \ref{app:defaultKernelParams}.

To ensure that the GPU is ready, the \texttt{initIfNeeded()} function at line \ref{code:executePtxWithParams:init} is called which creates a \textit{CUDA} context if one does not already exists.

At line \ref{code:executePtxWithParams:crtMd} a \textit{CUDA} module is created by calling \texttt{cuModuleLoadDataEx()}, which is part of the \textit{CUDA C} API. It takes \textit{PTX} code as a \texttt{C-string}, and outputs to the provided module container.

A function handle is needed in order to determine which function within the module to execute. The handle is identified by calling \texttt{cuModuleGetFunction()} at line \ref{code:executePtxWithParams:handle}. Here a function called \texttt{kernel} is requested from within the module. \texttt{kernel} is the hardcoded name of a function that is created by \textit{YAGAL}.

The module is executed on the GPU at line \ref{code:executePtxWithParams:launch} by a call to the function \texttt{cuLaunchKernel} and the module is then unloaded at line \ref{code:executePtxWithParams:clean} by \texttt{cuModuleUnload()}. The module have now been executed and the functionality within it is not needed anymore, therefore the module is unloaded.

\begin{lstlisting}[caption={int executePtxWithParams()}., label={code:executePtxWithParams}]
int executePtxWithParams(
        const std::string& ptx, 
        const std::vector<CUdeviceptr*>& kernelParams, 
        std::tuple<int,int,int> blockDimensions = {128, 1, 1}, 
        std::tuple<int,int,int> gridDimensions = {128, 1, 1}){
    CUmodule    cudaModule;
    CUfunction  function;
    CUlinkState linker;
    int         devCount;

    initIfNeeded(); ~\label{code:executePtxWithParams:init}~

    // Create module for object
    checkCudaErrors(
        cuModuleLoadDataEx(&cudaModule, ptx.c_str(), 0, 0, 0));~\label{code:executePtxWithParams:crtMd}~

    // Get kernel function
    checkCudaErrors(cuModuleGetFunction(
        &function, cudaModule, "kernel")); ~\label{code:executePtxWithParams:handle}~

    // Kernel launch
    _p.info() << "cuda kernel launching" << std::endl;
    checkCudaErrors(cuLaunchKernel(function, ~\label{code:executePtxWithParams:launch}~
        std::get<0>(gridDimensions), 
        std::get<1>(gridDimensions), 
        std::get<2>(gridDimensions),
        std::get<0>(blockDimensions), 
        std::get<1>(blockDimensions), 
        std::get<2>(blockDimensions),
        0, NULL, 
        (void**)kernelParams.data(), 
        NULL));

    // Cleanup
    checkCudaErrors(cuModuleUnload(cudaModule)); ~\label{code:executePtxWithParams:clean}~
    _p.info() << "cuda kernel executed successfully" 
        << std::endl;

    return 0;
}
\end{lstlisting}
