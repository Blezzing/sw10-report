\section{LLVM Optimizations}
\todo{Brug LLVMs erfaring - Det kan betale sig - redeg√∏r for det her}

% Motivations
LLVM contain tools for code generation, are have been developed throughout a long period of time. Y not use this, and stand atop the shoulders of giants?

% High-level IR
By using \textit{LLVM} we saved ourselves from a hurdle by not having to generate \textit{PTX} directly. \textit{LLVM IR}, while resembling \textit{assembly}, have a higher level of abstraction than \textit{PTX}. The process of generating code have therefore been made easier since we can stand atop the shoulders of \textit{LLVM} and let it handle the \textit{PTX} generation.

% optLevel
Another benefit of using \textit{LLVM} for generating \textit{PTX} is that \textit{LLVM} is probably better at optimizing \textit{PTX} than we can by hand. This is achieved simply by setting the code generation optimization flag which as \texttt{CodeGenOpt::Level optLevel(CodeGenOpt::Aggressive)}. Listing \ref{code::chainedAdds} shows a \textit{YAGAL} utilizing two chained \texttt{yagal::vector::add()} functions, which we will use as to showcase some of the achieved optimizations.

\begin{lstlisting}[caption={Chained \texttt{yagal::vector::add}}, label={code::chainedAdds}]
yagal::Vector<float> v({1.0, 2.0, 3.0});

v.add(5).add(5).exec();
\end{lstlisting}

\begin{figure}
  \hspace*{-.12\textwidth}%
  \begin{minipage}{0.6\textwidth}
    \centering
    \begin{lstlisting}[caption=code 2,frame=tlrb]{Name}
	.visible .entry kernel(
	.param .u64 kernel_param_0
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [kernel_param_0];
	mov.u32 	%r1, 0;
	st.u32 	[%SP+0], %r1;
	bra.uni 	LBB0_1;
LBB0_1:
	ld.u32 	%r2, [%SP+0];
	setp.lt.u32 	%p1, %r2, 3;
	@%p1 bra 	LBB0_4;
	bra.uni 	LBB0_3;
LBB0_2:
	mov.u32 	%r3, %nctaid.x;
	ld.u32 	%r4, [%SP+0];
	add.s32 	%r5, %r4, %r3;
	st.u32 	[%SP+0], %r5;
	bra.uni 	LBB0_1;
LBB0_3:
	ret;
LBB0_4:
	ld.s32 	%rd2, [%SP+0];
	shl.b64 	%rd3, %rd2, 2;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	add.rn.f32 	%f2, %f1, 0f40A00000;
	st.global.f32 	[%rd4], %f2;
	bra.uni 	LBB0_5;
LBB0_5:
	ld.s32 	%rd5, [%SP+0];
	shl.b64 	%rd6, %rd5, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f3, [%rd7];
	add.rn.f32 	%f4, %f3, 0f40800000;
	st.global.f32 	[%rd7], %f4;
	bra.uni 	LBB0_2;
}
	\end{lstlisting}
  \end{minipage}%
  \qquad
  \quad
  \begin{minipage}{0.6\textwidth}
    \centering
	\begin{lstlisting}[caption=code 2,frame=tlrb, firstnumber=1]{Name}
	.visible .entry kernel(
	.param .u64 kernel_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<4>;

	mov.u32 	%r5, 0;
	ld.param.u64 	%rd1, [kernel_param_0];
	mov.u32 	%r4, %nctaid.x;
	setp.lt.u32 	%p1, %r5, 3;
	@%p1 bra 	LBB0_3;
	bra.uni 	LBB0_2;
LBB0_3:
	mul.wide.s32 	%rd2, %r5, 4;
	add.s64 	%rd3, %rd1, %rd2;
	ld.global.f32 	%f1, [%rd3];
	add.rn.f32 	%f2, %f1, 0f40A00000;
	add.rn.f32 	%f3, %f2, 0f40800000;
	st.global.f32 	[%rd3], %f3;
	add.s32 	%r5, %r5, %r4;
	setp.lt.u32 	%p1, %r5, 3;
	@%p1 bra 	LBB0_3;
LBB0_2:
	ret;
}
	\end{lstlisting}
  \end{minipage}%
  \hspace*{-.1\textwidth}%
\end{figure}