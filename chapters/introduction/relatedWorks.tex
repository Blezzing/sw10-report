\section{Related Works}
We consider related abstraction libraries in the domain of heterogeneous computing. This is done to identify relevant key points that can help define the library we are developing.

The libraries are being considering in regards to:
\begin{description}
\item[Goals] \hfill \\
To identify the motivation for the library and to understand the motivation behind its design choices.
\item[Programming Model] \hfill \\
To identify the programming model of a library to see what aproaches have been tried, and what have been made possible so far.
\item[Implementation] \hfill \\
To identify the means of facilitating the programming model, showing how it can be done.
\item[Cognitive Dimensions of Notations] \hfill \\
To get a general idea of strengths and weaknesses of the library.
\item[Key Points] \hfill \\
To identify relevant points to note from a library, that should be considered when designing our library.
\end{description}

\todo[inline]{Argumenter for hvorfor vi har valgt at se på disse libraries}
 
\subsection{Thrust}
Thrust is a template library indended to allow developers to implement high performance applications with minimal programming effort. This section is based on Thrust's overview document\cite{thrustOverview} and github page\cite{thrustGithub}.

\subsubsection{Goals}
Thrust is intended to make high performance application development as easy as possible. It is designed to be similar to STL, with intention of being concise, readable, and efficient. It is inteded to supply the developers with containers and fundamental algorithms, with user defined behavior, rather than specific numeric algorithms as provided by BLAS. It is also intended to be interoperable with CUDA.

\subsubsection{Programming Model}
Bolt is modeled on STL and as such, follows the model of calling functions with iterators as arguments to instruct where input, and output is located.

We here show how saxpy can be implemented in Thrust, and the usage of iterators to manage data access is shown.
\begin{lstlisting}
size_t N = 1024;
int a = 10;

//cuda classifier on lambdaz
auto func = [=]__device__(int x, int y){return a * x + y;};

//initialize host vectors
thrust::host_vector<int> x(N);
thrust::host_vector<int> y(N);

//fill with random data
std::generate(x.begin(), x.end(), rand);
std::generate(y.begin(), y.end(), rand);

//copy to device
thrust::device_vector<int> d_x = x;
thrust::device_vector<int> d_y = y;

//perform saxpy
thrust::transform(d_x.begin(), d_x.end(), d_y.begin(), d_y.begin(), func);

//copy results back to host vector
y = d_y;
\end{lstlisting}

\subsubsection{Implementation}
Thrust is a library with abstractions build on top of CUDA.

\subsubsection{Cognitive Dimensions of Notations}

\subsubsection{Key Points}
The key points of Thrust is:

\begin{itemize}
\item API immitating STL.
\item Being purely a library on top of CUDA.
\item Allowing mixing with CUDA code. 
\end{itemize}

The usage of the containers is based on iterators, as STL is, and can be very verbose with multiple operations on the same container.

\subsection{SkelCL}
SkelCL (Skeleton Computing Language) is a library which aim is to provide abstractions for parallel programming on multi GPU systems. It is developed as a research project by Michel Steuwer et.al at University of Munster. This section is witten based upon the information available on their website \cite{skelclWebsite} and their paper \cite{skelclPaper}.

\subsubsection{Goals}
The developers of SkelCL believes that programming for GPUs result in complex, lengthy and error prone programs. This is due to the process of writing GPU code typically is reliant on low level programming aproaches as seen with OpenCL and CUDA. 

To avoid the pitfalls of the traditional low level aproaches, the library SkelCL provide abstractions in the form of algorithmic patterns, parallel container data types, and handling of transfers between host and device. 

SkelCl can be used on single GPU systems, but is mainly aimed at systems woth multipe GPUs and provide a feature called \textit{data (re)distributions} which manages data among the available GPUs.

\subsubsection{Programming model}
The programming model is centered around \textit{parallel skeletons}, which is pre-implemented high level patterns that can be customized for a given problem. The available skeletons are \textit{map}, \textit{zip}, \textit{reduce}, \textit{scan}, \textit{mapOverlap}, end \textit{allpairs}.

A computation of the dot product of two vectors in SkelCL is shown in listing \ref{code:skelclSample}. After SkelCL is initialised, line three, the skeletons can be constructed. The \texttt{Zip} and \texttt{Reduce} have been used and are specified by a the provided paremeters; \texttt{<int(int,int)>} indicates that the function expects two integers and a single integer will be returned. The given string specifies the function of the skeleton. In line 12 the calculation is performed based on the constructed skeletons.

\begin{lstlisting}[caption={Computation of the dot product of two vectors}, label=code:skelclSample] 
using namespace skelcl;
int main() {
  skelcl::init();

  Zip<int(int,int)> mult("int func(int x, int y){ return x*y; }");
  Reduce<int(int)> sum("int func(int x, int y){ return x+y; }", "0");

  Vector<int> a(1024);
  Vector<int> b(1024);
  init(a.begin(), a.end()); init(b.begin(), b.end());

  Vector<int> c = sum( mult(a, b) );

  std::cout << "dot product: " << c.front() << std::endl;
}
\end{lstlisting}

\subsubsection{Implementation}
SkelCL is a library that is built upon OpenCL. This allows host and kernel code to be contained within one source file, as opposed to the traditional OpenCL approach.

\subsubsection{Cognitive Dimensions of Notations}
\todo[inline]{Empty for now}

\subsubsection{Key points}
A key point of OpenCL is the data containers it provide, namely vectors and matrices. They are transparently available on both host and device. When one of these data containers are allocated or deallocated on the host, it is automatically also allocated or dealocated on the device(s). Futhermore, memory transfers between host and device are handled implicitly.

Another key point of SkelCL is how it is designed to function on systems with multiple GPUs. The \textit{disctribution mechanism} that OpenCL provides describes how a container is distributed among the available GPUs. This feature abstracts away the need to manage what parts of the container gets assigned to which GPU. The data containers can instead be seen as self contained entities. The programmer instead have specify a model for how the data should be distributed, with the available options being \textit{single}, \textit{copy}, \textit{block}, and \textit{overlap}.

\subsection{Bolt}
Bolt is a library providing abstractions for heterogeneous computing. This section is based on Bolt's documentation\cite{boltDoc} and github page\cite{boltGithub}.

An unique feature of Bolt is the possibility to run its algoritms on either CPU or GPU without modifications in the code.

\subsubsection{Goals}
Bolt is designed to provide high performance library implementations for common algorithms, following the structure of STL. It is intended to make heterogeneous development easier.

It is designed to provide an application that can execute on either a CPU or any OpenCL capable unit.

\subsubsection{Programming Model}
Bolt is modeled on STL and as such, follows the model of calling functions with iterators as arguments to instruct where input, and output is located.

An example is shown in Listing \ref{code:boltExample1}, where we sort a device vector. This is identical to the method shown in Listing \ref{code:boltExample2}, where a a std::vector is sorted with std::sort.
\begin{lstlisting}[caption={Bolt sort example}, label={code:boltExample1}]
//vector construction
bolt::cl::device_vector<int> input(1024);

//vector fill ommited

//inplace sort
bolt::cl::sort(input.begin(), input.end());
\end{lstlisting}

\begin{lstlisting}[caption={STL sort example}, label={code:boltExample2}]
//vector construction
std::vector<int> input(1024);

//vector fill ommited

//inplace sort
std::sort(input.begin(), input.end());
\end{lstlisting}

\subsubsection{Implementation}
\todo[inline]{Bolt is a library... Er ikke sikker på om det kræver speciel kompilerings process.

Bolt targets... almindelige cpuer og alt der kan køre c++ amp, eller openCL.
}

\subsubsection{Cognitive Dimensions of Notations}
\todo[inline]{we could evaluate all criterion here}

\subsubsection{Key Points}
The key points of Bolt is:

\begin{itemize}
\item The immitation of STL.
\item The capability of single code base working on both CPU and GPU.
\end{itemize}

The usage of the containers is based on iterators, as STL is, and can be very verbose with multiple operations on the same container.

% ---=== CyCL ===--- %

\subsection{CYCL}

\subsubsection{Goals}

\subsubsection{Programming model}

\subsubsection{Implementation}

\subsection{Cognitive Dimensions of Notations}

\subsubsection{Key Points}

% ---=== C++ AMP ===--- %

\subsection{C++ AMP}

\subsubsection{Goals}

\subsubsection{Programming model}

\subsubsection{Implementation}

\subsection{Cognitive Dimensions of Notations}

\subsubsection{Key Points}

% ---=== PACXX ===--- %

\subsection{PACXX}

\subsubsection{Goals}

\subsubsection{Programming model}

\subsubsection{Implementation}

\subsection{Cognitive Dimensions of Notations}

\subsubsection{Key Points}