\section{Project Prerequisites}
In our previous semester project\cite{sw9Report}, we compared multiple languages with frameworks supporting development targeting the GPU. We observed how \textit{CUDA} and \textit{OpenCL} are the frameworks that offer the most explicit device control, and as such can be the choice for developers with knowledge of how to fine tune a GPGPU application.

Based on our previous experience we take some early choices, that set the direction of the project.

\subsection{Language Selection}\label{cha:languageSelection}
Our goal of this project is the construction of a framework that can assist in providing applications with an initial benefit of GPGPU acceleration, where our framework is replaceable by another framework with a more low level programming model when the developer is prepared for the steep learning curve of traditional GPGPU development. It is therefore convenient to keep our framework in the same language these low level approaches. As \textit{CUDA} and \textit{OpenCL} are used from \textit{C++} and \textit{C} we want to look at these. The abstractions allowed in \textit{C++}, in the form of templates and lambdas, makes it more attractive for us, as we want to provide high level abstractions. As such \textit{C++} is our language of choice, and the framework we create in this project will be based on \textit{C++}.

\subsection{Platform}
Generating code to execute on a GPU can be done on multiple levels, either targeting a high level languages like \textit{CUDA}, or assembly like languages such as \textit{PTX}. Targeting a high level language gives the benefit of the compilation tool chain being able to perform optimizations, while targeting an assembly like language gives more explicit control.

\textit{LLVM} is a collection of compiler tools and technologies, which can be used to construct compilers and other tool chains for specific needs. One of the technologies of \textit{LLVM} is the \textit{LLVM Intermediate Representation} language, which can be constructed using \textit{LLVM} tools, being a platform agnostic language that can be compiled to platform specific code for any supported platform. A platform that is supported is \textit{NVIDIA GPU}s, through the \textit{NVPTX Back End}, which allows us to compile \textit{LLVM Intermediate Representation} to \textit{PTX} code. Targeting \textit{LLVM Intermediate Representation} allows us to generate code on a higher abstraction level than \textit{PTX}, with optimizations provided by \textit{LLVM}. Compared to a high level language such as CUDA, we get more explicit control over the execution, wo we choose to target \textit{LLVM Intermediate Representation}, as it seem to be of a convenient abstraction level between \textit{CUDA} and \textit{PTX}, and as it allows for expanding to other platforms than \textit{NVIDIA GPU}s based on the available \textit{LLVM Back Ends}.

Using \textit{LLVM Intermediate Representation} as the target platform, we can generate \textit{PTX} code. To launch kernels written in \textit{PTX} we need to use the \textit{CUDA Driver API}. Memory management on the device also need to be done through this API, as it is done outside kernels. This decision requires YAGAL to be used on a system with a \textit{NVIDIA GPU}, but can allow for easier transition for the developer into \textit{CUDA} as YAGAL constructs use \textit{CUDA} constructs.